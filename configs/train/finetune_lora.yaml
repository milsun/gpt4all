# model/tokenizer
model_name: "decapoda-research/llama-65b-hf"
tokenizer_name: "decapoda-research/llama-65b-hf"
gradient_checkpointing: true
save_name: "milsunone/llama-65b-lora-flash-attention-python"
max_tokens: 2048

# dataset
streaming: false
num_proc: 64
dataset_path: "data"
max_length: 2048
batch_size: 128
micro_batch_size: 4

# train dynamics
lr: 3.0e-4
eval_every: 2000
eval_steps: 100
save_every: 2000
output_dir: "ckpts/llama-lora-flash-attention-python"
checkpoint: null
warmup_steps: 100
num_epochs: 1

# lora 
lora: true
lora_rank: 64
lora_alpha: 32
lora_dropout: 0.05

# logging
wandb: true
wandb_entity: # update
wandb_project_name: "llama-65b-lora-flash-attention-python"
seed: 69
